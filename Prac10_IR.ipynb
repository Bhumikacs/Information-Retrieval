{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Practical 10**\n",
        "\n",
        "**Aim : Advanced Topics in Information Retrieval**\n",
        "*   Implement a text summarization algorithm (e.g., extractive or abstractive).\n",
        "*   Build a question-answering system using techniques such as information\n",
        "extraction.\n",
        "\n"
      ],
      "metadata": {
        "id": "tYXNPpAA_xU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A) Implement a text summarization algorithm (e.g., extractive or abstractive).**"
      ],
      "metadata": {
        "id": "QRpMqGczmAyv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChAGFS08_gus",
        "outputId": "1515a3f2-3ede-4e12-dc2e-bcbca848b922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T114 | Bhumika Shelar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary:\n",
            " \n",
            "Information Retrieval is the process of obtaining relevant information\n",
            "from a large collection of data. Text summarization helps in reducing the size of documents while preserving\n",
            "important information.\n"
          ]
        }
      ],
      "source": [
        "print(\"T114 | Bhumika Shelar\")\n",
        "# Import required libraries\n",
        "import nltk\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Download NLTK resources (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Added to resolve LookupError\n",
        "\n",
        "# Input text\n",
        "text = \"\"\"\n",
        "Information Retrieval is the process of obtaining relevant information\n",
        "from a large collection of data. It plays an important role in search engines.\n",
        "Text summarization helps in reducing the size of documents while preserving\n",
        "important information. Automatic summarization is widely used in news\n",
        "applications and research domains.\n",
        "\"\"\"\n",
        "\n",
        "# Sentence tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Remove stop words and calculate word frequencies\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_frequencies = {}\n",
        "\n",
        "for word in word_tokenize(text.lower()):\n",
        "    if word.isalnum() and word not in stop_words:\n",
        "        if word not in word_frequencies:\n",
        "            word_frequencies[word] = 1\n",
        "        else:\n",
        "            word_frequencies[word] += 1\n",
        "\n",
        "# Normalize word frequencies\n",
        "max_frequency = max(word_frequencies.values())\n",
        "for word in word_frequencies:\n",
        "    word_frequencies[word] /= max_frequency\n",
        "\n",
        "# Score sentences\n",
        "sentence_scores = {}\n",
        "for sentence in sentences:\n",
        "    for word in word_tokenize(sentence.lower()):\n",
        "        if word in word_frequencies:\n",
        "            if sentence not in sentence_scores:\n",
        "                sentence_scores[sentence] = word_frequencies[word]\n",
        "            else:\n",
        "                sentence_scores[sentence] += word_frequencies[word]\n",
        "\n",
        "# Select top 2 sentences\n",
        "summary_sentences = sorted(\n",
        "    sentence_scores,\n",
        "    key=sentence_scores.get,\n",
        "    reverse=True\n",
        ")[:2]\n",
        "\n",
        "# Generate summary\n",
        "summary = ' '.join(summary_sentences)\n",
        "print(\"Summary:\\n\", summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B) Build a question-answering system using techniques such as information extraction.**"
      ],
      "metadata": {
        "id": "JDNCAlaEmH5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"T114 | Bhumika Shelar\")\n",
        "# Import required libraries\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Load spaCy language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input document\n",
        "text = \"\"\"\n",
        "Information Retrieval deals with the storage and retrieval of information.\n",
        "Search engines use IR techniques to retrieve relevant documents.\n",
        "Natural Language Processing helps computers understand human language.\n",
        "\"\"\"\n",
        "\n",
        "# Sentence tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Accept user question\n",
        "question = input(\"Enter your question: \")\n",
        "\n",
        "# Extract keywords from question\n",
        "question_doc = nlp(question)\n",
        "keywords = [\n",
        "    token.text.lower()\n",
        "    for token in question_doc\n",
        "    if not token.is_stop and not token.is_punct\n",
        "]\n",
        "\n",
        "# Find most relevant sentence\n",
        "best_sentence = \"\"\n",
        "max_score = 0\n",
        "\n",
        "for sentence in sentences:\n",
        "    score = 0\n",
        "    sentence_doc = nlp(sentence.lower())\n",
        "\n",
        "    for token in sentence_doc:\n",
        "        if token.text in keywords:\n",
        "            score += 1\n",
        "\n",
        "    if score > max_score:\n",
        "        max_score = score\n",
        "        best_sentence = sentence\n",
        "\n",
        "# Display answer\n",
        "if best_sentence:\n",
        "    print(\"Answer:\", best_sentence)\n",
        "else:\n",
        "    print(\"Answer not found in the document.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9LNb-VflYWE",
        "outputId": "b0840aba-89ce-4119-f105-537612af2129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T114 | Bhumika Shelar\n",
            "Enter your question: Which techniques search engine uses?\n",
            "Answer: Search engines use IR techniques to retrieve relevant documents.\n"
          ]
        }
      ]
    }
  ]
}